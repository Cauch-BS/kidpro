{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import json\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(f\"현재 사용 가능한 GPU 개수: {torch.cuda.device_count()}\")\n",
    "print(f\"현재 GPU 이름: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "LABEL_CSV = Path(\"/home/khdp-user/workspace/dataset/CSV/m0m1_label.csv\")\n",
    "PATCH_ROOT = Path(\"/home/khdp-user/workspace/dataset/Glom_M0M1\")\n",
    "PATCH_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "OUT_DIR = Path(\"/home/khdp-user/workspace/m0m1_run_cls\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_PATH = os.path.join(OUT_DIR, \"dataset.csv\")\n",
    "BEST_MODEL_PATH = OUT_DIR / \"best_model.pt\"\n",
    "SPLIT_CSV_PATH  = OUT_DIR / \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_index(root: Path, exts=(\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")):\n",
    "    idx = {}\n",
    "    for ext in exts:\n",
    "        for p in root.rglob(f\"*{ext}\"):\n",
    "            idx[p.name] = p\n",
    "    return idx\n",
    "\n",
    "print(\"[Index] scanning patches...\")\n",
    "patch_index = build_patch_index(PATCH_ROOT)\n",
    "print(\"Total patches indexed:\", len(patch_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "rows = []\n",
    "missing = 0\n",
    "for _, r in label_df.iterrows():\n",
    "    name = r[\"patch_name\"]\n",
    "    p = patch_index.get(name)\n",
    "    if p is None:\n",
    "        missing += 1\n",
    "        continue\n",
    "\n",
    "    y = 1 if r[\"target\"].lower() == \"m1\" else 0\n",
    "    rows.append({\n",
    "        \"name\": name,\n",
    "        \"path\": str(p),\n",
    "        \"y\": y\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"df shape:\", df.shape, \"missing:\", missing)\n",
    "print(df[\"y\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slide_id_from_patch_name(patch_name: str):\n",
    "    return patch_name.split(\"_PAS\")[0]\n",
    "\n",
    "def stratified_split_slide(\n",
    "    slide_df,\n",
    "    y_col=\"slide_y\",\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    seed=42,\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    slide_split = {}\n",
    "\n",
    "    for cls, sub in slide_df.groupby(y_col):\n",
    "        slide_ids = sub[\"slide_id\"].values\n",
    "        rng.shuffle(slide_ids)\n",
    "\n",
    "        n = len(slide_ids)\n",
    "        n_tr = int(n * train_ratio)\n",
    "        n_va = int(n * val_ratio)\n",
    "\n",
    "        for sid in slide_ids[:n_tr]:\n",
    "            slide_split[sid] = \"train\"\n",
    "        for sid in slide_ids[n_tr:n_tr+n_va]:\n",
    "            slide_split[sid] = \"val\"\n",
    "        for sid in slide_ids[n_tr+n_va:]:\n",
    "            slide_split[sid] = \"test\"\n",
    "\n",
    "    return slide_split\n",
    "\n",
    "\n",
    "df[\"slide_id\"] = df[\"name\"].apply(get_slide_id_from_patch_name)\n",
    "\n",
    "slide_df = (\n",
    "    df.groupby(\"slide_id\")[\"y\"]\n",
    "      .max()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"y\": \"slide_y\"})\n",
    ")\n",
    "\n",
    "slide_split = stratified_split_slide(slide_df)\n",
    "\n",
    "df[\"split\"] = df[\"slide_id\"].map(slide_split)\n",
    "assert df[\"split\"].isna().sum() == 0\n",
    "\n",
    "print(\"Patch-level distribution\")\n",
    "print(pd.crosstab(df[\"split\"], df[\"y\"]))\n",
    "\n",
    "slide_view = (\n",
    "    df[[\"slide_id\", \"split\"]]\n",
    "    .drop_duplicates(\"slide_id\")\n",
    ")\n",
    "print(\"\\nSlide-level distribution (#slides)\")\n",
    "print(slide_view[\"split\"].value_counts())\n",
    "\n",
    "slide_view = slide_df.merge(\n",
    "    df[[\"slide_id\", \"split\"]].drop_duplicates(\"slide_id\"),\n",
    "    on=\"slide_id\"\n",
    ")\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"[OK] CSV saved: {CSV_PATH}  (patches={len(df)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchClsDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = cv2.imread(row[\"path\"])\n",
    "        if img is None:\n",
    "            raise RuntimeError(row[\"path\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "\n",
    "        y = torch.tensor(row[\"y\"]).long()\n",
    "        return img, y\n",
    "    \n",
    "def get_transforms():\n",
    "    train_tf = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Resize(PATCH_SIZE, PATCH_SIZE),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    val_tf = A.Compose([\n",
    "        A.Resize(PATCH_SIZE, PATCH_SIZE),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, mode=\"min\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "\n",
    "        improved = (\n",
    "            score < self.best_score - self.min_delta\n",
    "            if self.mode == \"min\"\n",
    "            else score > self.best_score + self.min_delta\n",
    "        )\n",
    "\n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = timm.create_model(\n",
    "        \"resnet50\",\n",
    "        pretrained=True,\n",
    "        num_classes=2 \n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "    val_loss = total_loss / max(n, 1)\n",
    "    val_acc  = correct / max(n, 1)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train(df):\n",
    "    train_tf, val_tf = get_transforms()\n",
    "\n",
    "    df_tr = df[df.split == \"train\"]\n",
    "    df_va = df[df.split == \"val\"]\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        PatchClsDataset(df_tr, train_tf),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        PatchClsDataset(df_va, val_tf),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = build_model()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    early_stopper = EarlyStopping(\n",
    "        patience=5,\n",
    "        min_delta=1e-4,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # ==================\n",
    "        # Train\n",
    "        # ==================\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        pbar = tqdm(dl_tr, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        # ==================\n",
    "        # Validation\n",
    "        # ==================\n",
    "        val_loss, val_acc = validate(model, dl_va, criterion)\n",
    "\n",
    "        # ==================\n",
    "        # Early Stopping\n",
    "        # ==================\n",
    "        is_best = early_stopper.step(val_loss)\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "        # ==================\n",
    "        # Logging\n",
    "        # ==================\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "            f\"train_loss={train_loss:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} | \"\n",
    "            f\"val_acc={val_acc:.4f} | \"\n",
    "            f\"best_val_loss={early_stopper.best_score:.4f} | \"\n",
    "            f\"patience={early_stopper.counter}/{early_stopper.patience}\"\n",
    "        )\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"[Early Stop] Training stopped.\")\n",
    "            break\n",
    "\n",
    "    print(f\"[DONE] Best model saved to {BEST_MODEL_PATH}\")\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_env(out_dir):\n",
    "    env = {\n",
    "        \"TASK_TYPE\": 'classification',\n",
    "        \"PATCH_SIZE\": PATCH_SIZE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"LR\": LR,\n",
    "        \"TEST_RATIO\": 0.1,\n",
    "        \"VAL_RATIO\": 0.1,\n",
    "        \"target_mag\": 10.0,\n",
    "        \"DEVICE\": DEVICE,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"python_version\": platform.python_version(),\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(out_dir, \"training_env.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(env, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Training environment saved: {save_path}\")\n",
    "save_training_env(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bonryu]",
   "language": "python",
   "name": "conda-env-bonryu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
