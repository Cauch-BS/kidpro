{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import platform\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Datasets split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "ROOT_DIR = \"/home/khdp-user/workspace/Infla_patch\"\n",
    "TASK_TYPE = \"binary\"      # \"binary\" | \"multiclass\"\n",
    "LAYER_IDS = [1]           # binary면 Len == 1\n",
    "PATCH_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "TEST_RATIO = 0.2\n",
    "VAL_RATIO = 0.2   # train 중 val 비율\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUT_DIR = \"/home/khdp-user/workspace/Infla_run_seg\"\n",
    "CSV_PATH = os.path.join(OUT_DIR, \"dataset.csv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "assert TASK_TYPE in [\"binary\", \"multiclass\"]\n",
    "if TASK_TYPE == \"binary\":\n",
    "    assert len(LAYER_IDS) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET SPLIT\n",
    "# ============================================================\n",
    "def build_dataset_csv():\n",
    "    root = Path(ROOT_DIR)\n",
    "    slides = sorted([p for p in root.iterdir() if p.is_dir()])\n",
    "    slide_ids = [s.name for s in slides]\n",
    "\n",
    "    random.shuffle(slide_ids)\n",
    "\n",
    "    n = len(slide_ids)\n",
    "    n_test = int(n * TEST_RATIO)\n",
    "    n_val = int((n - n_test) * VAL_RATIO)\n",
    "\n",
    "    test_slides = set(slide_ids[:n_test])\n",
    "    val_slides = set(slide_ids[n_test:n_test+n_val])\n",
    "    train_slides = set(slide_ids[n_test+n_val:])\n",
    "\n",
    "    def split_of(slide):\n",
    "        if slide in train_slides: return \"train\"\n",
    "        if slide in val_slides: return \"val\"\n",
    "        return \"test\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for slide in slides:\n",
    "        slide_id = slide.name\n",
    "        split = split_of(slide_id)\n",
    "\n",
    "        img_dir = slide / \"images\"\n",
    "        mask_root = slide / \"masks\"\n",
    "\n",
    "        for img_path in img_dir.glob(\"*.png\"):\n",
    "            # LAYER_IDS 중 하나라도 mask 있으면 포함\n",
    "            valid = False\n",
    "            for lid in LAYER_IDS:\n",
    "                mp = mask_root / f\"layer{lid}\" / img_path.name\n",
    "                if mp.exists():\n",
    "                    valid = True\n",
    "                    break\n",
    "            if not valid:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"name\": img_path.name,\n",
    "                \"path\": str(img_path),\n",
    "                \"split\": split,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"[OK] CSV saved: {CSV_PATH}  (patches={len(df)})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_dataset_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(CSV_PATH).value_counts('split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "class SegDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = cv2.imread(row[\"path\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        slide_dir = Path(row[\"path\"]).parents[1]\n",
    "        mask_root = slide_dir / \"masks\"\n",
    "\n",
    "        masks = []\n",
    "        for lid in LAYER_IDS:\n",
    "            mp = mask_root / f\"layer{lid}\" / row[\"name\"]\n",
    "            if mp.exists():\n",
    "                m = cv2.imread(str(mp), 0) > 127\n",
    "            else:\n",
    "                m = np.zeros(img.shape[:2], bool)\n",
    "            masks.append(m)\n",
    "\n",
    "        if TASK_TYPE == \"binary\":\n",
    "            mask = masks[0].astype(np.float32)\n",
    "        else:\n",
    "            # multiclass (background=0)\n",
    "            mask = np.zeros(img.shape[:2], np.int64)\n",
    "            for i, m in enumerate(masks):\n",
    "                mask[m] = i + 1\n",
    "\n",
    "        if self.transform:\n",
    "            out = self.transform(image=img, mask=mask)\n",
    "            img, mask = out[\"image\"], out[\"mask\"]\n",
    "        \n",
    "        img = img.float()\n",
    "        if TASK_TYPE == \"binary\":\n",
    "            mask = mask.float()     \n",
    "        else:\n",
    "            mask = mask.long()\n",
    "        return img, mask\n",
    "\n",
    "    \n",
    "# ============================================================\n",
    "# TRANSFORMS\n",
    "# ============================================================\n",
    "def get_transforms():\n",
    "    train_tf = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Normalize(),\n",
    "        A.Resize(PATCH_SIZE,PATCH_SIZE),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    val_tf = A.Compose([\n",
    "        A.Resize(PATCH_SIZE,PATCH_SIZE),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING\n",
    "# ============================================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, mode=\"min\"):\n",
    "        \"\"\"\n",
    "        patience : 개선 없이 기다릴 epoch 수\n",
    "        min_delta: 개선으로 인정할 최소 변화량\n",
    "        mode     : 'min' (loss) or 'max' (metric)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return True  # best 갱신\n",
    "\n",
    "        improved = (\n",
    "            score < self.best_score - self.min_delta\n",
    "            if self.mode == \"min\"\n",
    "            else score > self.best_score + self.min_delta\n",
    "        )\n",
    "\n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    dice_list = []\n",
    "    iou_list = []\n",
    "\n",
    "    for img, mask in loader:\n",
    "        img, mask = img.to(DEVICE), mask.to(DEVICE)\n",
    "        pred = model(img)\n",
    "\n",
    "        if TASK_TYPE == \"binary\":\n",
    "            loss = criterion(pred.squeeze(1), mask)\n",
    "            prob = torch.sigmoid(pred)\n",
    "            pred_bin = (prob > 0.5).float()\n",
    "            gt = mask.unsqueeze(1)\n",
    "        else:\n",
    "            loss = criterion(pred, mask)\n",
    "            prob = torch.softmax(pred, dim=1)\n",
    "            pred_bin = torch.argmax(prob, dim=1)\n",
    "            gt = mask\n",
    "\n",
    "        total_loss += loss.item() * img.size(0)\n",
    "        n += img.size(0)\n",
    "\n",
    "        # ---- Dice / IoU (batch 평균)\n",
    "        if TASK_TYPE == \"binary\":\n",
    "            p = pred_bin.view(pred_bin.size(0), -1)\n",
    "            g = gt.view(gt.size(0), -1)\n",
    "\n",
    "            inter = (p * g).sum(dim=1)\n",
    "            union = p.sum(dim=1) + g.sum(dim=1)\n",
    "\n",
    "            dice = (2 * inter + 1e-6) / (union + 1e-6)\n",
    "            iou = (inter + 1e-6) / (p.sum(dim=1) + g.sum(dim=1) - inter + 1e-6)\n",
    "\n",
    "            dice_list.append(dice.mean().item())\n",
    "            iou_list.append(iou.mean().item())\n",
    "        else:\n",
    "            num_classes = pred.shape[1]\n",
    "            dice_per_class = []\n",
    "            iou_per_class = []\n",
    "\n",
    "            for c in range(1, num_classes):  # background 제외\n",
    "                p = (pred_bin == c).float().view(pred_bin.size(0), -1)\n",
    "                g = (gt == c).float().view(gt.size(0), -1)\n",
    "\n",
    "                inter = (p * g).sum(dim=1)\n",
    "                union = p.sum(dim=1) + g.sum(dim=1)\n",
    "\n",
    "                dice = (2 * inter + 1e-6) / (union + 1e-6)\n",
    "                iou  = (inter + 1e-6) / (p.sum(dim=1) + g.sum(dim=1) - inter + 1e-6)\n",
    "\n",
    "                dice_per_class.append(dice.mean())\n",
    "                iou_per_class.append(iou.mean())\n",
    "            dice_list.append(torch.stack(dice_per_class).mean().item())\n",
    "            iou_list.append(torch.stack(iou_per_class).mean().item())\n",
    "\n",
    "\n",
    "\n",
    "    val_loss = total_loss / max(n, 1)\n",
    "    val_dice = float(np.mean(dice_list)) if dice_list else 0.0\n",
    "    val_iou  = float(np.mean(iou_list)) if iou_list else 0.0\n",
    "\n",
    "    return val_loss, val_dice, val_iou\n",
    "\n",
    "\n",
    "def train_model(df):\n",
    "    train_tf, val_tf = get_transforms()\n",
    "\n",
    "    df_tr = df[df.split == \"train\"]\n",
    "    df_va = df[df.split == \"val\"]\n",
    "\n",
    "    ds_tr = SegDataset(df_tr, train_tf)\n",
    "    ds_va = SegDataset(df_va, val_tf)\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # model / loss\n",
    "    if TASK_TYPE == \"binary\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=\"resnet50\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "        criterion = smp.losses.DiceLoss(mode=\"binary\", from_logits=True)\n",
    "    else:\n",
    "        model = smp.Unet(\n",
    "            encoder_name=\"resnet50\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=len(LAYER_IDS) + 1,\n",
    "            activation=None,\n",
    "        )\n",
    "        criterion = smp.losses.DiceLoss(mode=\"multiclass\", from_logits=True)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    early_stopper = EarlyStopping(patience=5, min_delta=1e-5, mode=\"min\")\n",
    "    best_path = os.path.join(OUT_DIR, \"best_model.pt\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        # ==================\n",
    "        # Train\n",
    "        # ==================\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        pbar = tqdm(dl_tr, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for img, mask in pbar:\n",
    "            img, mask = img.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "            pred = model(img)\n",
    "            if TASK_TYPE == \"binary\":\n",
    "                loss = criterion(pred.squeeze(1), mask)\n",
    "            else:\n",
    "                loss = criterion(pred, mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "        # ==================\n",
    "        # Validation\n",
    "        # ==================\n",
    "        val_loss, val_dice, val_iou = validate(model, dl_va, criterion)\n",
    "        # ==================\n",
    "        # Early Stopping\n",
    "        # ==================\n",
    "        is_best = early_stopper.step(val_loss)\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "        # ==================\n",
    "        # Logging (핵심)\n",
    "        # ==================\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "            f\"train_loss={train_loss:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} | \"\n",
    "            f\"val_dice={val_dice:.4f} | \"\n",
    "            f\"val_iou={val_iou:.4f} | \"\n",
    "            f\"best={early_stopper.best_score:.4f} | \"\n",
    "            f\"patience={early_stopper.counter}/{early_stopper.patience}\"\n",
    "        )\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"[Early Stop] Training stopped.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f\"[DONE] Best model saved to {best_path}\")\n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_env(out_dir):\n",
    "    env = {\n",
    "        \"TASK_TYPE\": TASK_TYPE,\n",
    "        \"LAYER_IDS\": LAYER_IDS,\n",
    "        \"PATCH_SIZE\": PATCH_SIZE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"LR\": LR,\n",
    "        \"TEST_RATIO\": TEST_RATIO,\n",
    "        \"VAL_RATIO\": VAL_RATIO,\n",
    "        \"target_mag\": 10.0,\n",
    "        \"DEVICE\": DEVICE,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"python_version\": platform.python_version(),\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(out_dir, \"training_env.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(env, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Training environment saved: {save_path}\")\n",
    "save_training_env(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
